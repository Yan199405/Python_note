## 明确目标
爬取https://www.runoob.com/w3cnote/scrapy-detail.html网站所有老师的姓名、职称和个人信息。

## 新建scrapy的爬虫项目
1. 创建项目

    - scrapy startproject Myspider
    
2. 创建爬虫文件:spiders/itcast.py,爬取域的范围itcast.cn

    - scrapy genspider itcast itcast.cn
    - 爬虫文件内容解析
    
            import scrapy
                 
            class ItcastSpider(scrapy.Spider):
                name = 'itcast'                  
                allowed_domains = ['itcast.cn']
                start_urls = ['http://itcast.cn/']
            
                def parse(self, response):
                    pass
            //name: 这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。
            //allow_domains = [] 是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。
            //start_urls = () ：爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。
            //parse(self, response) ：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下
                负责解析返回的网页数据(response.body)，提取结构化数据(生成item)
                生成需要下一页的URL请求。
- 修改爬虫文件
    - 将start_urls的值修改为需要爬取的第一个url
    
            start_urls = ("http://www.itcast.cn/channel/teacher.shtml",)  
        
    - 修改parse()方法

            def parse(self, response):
                filename = 'teacher.html'
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(response.text)       
        
    - 运行
        
        >scrapy crawl itcast
        
        //itcast是spider的name
        //成功生成源代码到teacher.html则表示运行成功
            
- items文件中添加需要爬取的内容
    - 如下    
        
          class MyspiderItem(scrapy.Item):
              # define the fields for your item here like:
              # name = scrapy.Field()
              name = scrapy.Field()
              title = scrapy.Field()
              info = scrapy.Field()    
        
- 选择器取数据   
        
        
        
        
        
        
        
        
        
        
        
        
        
               