## 明确目标
爬取https://www.runoob.com/w3cnote/scrapy-detail.html网站所有老师的姓名、职称和个人信息。

## 新建scrapy的爬虫项目
1. 创建项目

    - scrapy startproject Myspider
    
2. 创建爬虫文件:spiders/itcast.py,爬取域的范围itcast.cn

    - scrapy genspider itcast itcast.cn
    - 爬虫文件内容解析
    
            import scrapy
                 
            class ItcastSpider(scrapy.Spider):
                name = 'itcast'                  
                allowed_domains = ['itcast.cn']
                start_urls = ['http://itcast.cn/']
            
                def parse(self, response):
                    pass
            //name: 这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。
            //allow_domains = [] 是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。
            //start_urls = () ：爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。
            //parse(self, response) ：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下
                负责解析返回的网页数据(response.body)，提取结构化数据(生成item)
                生成需要下一页的URL请求。
- 修改爬虫文件
    - 将start_urls的值修改为需要爬取的第一个url
    
            start_urls = ("http://www.itcast.cn/channel/teacher.shtml",)  
        
    - 修改parse()方法

            def parse(self, response):
                filename = 'teacher.html'
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(response.text)       
        
    - 运行
        
        >scrapy crawl itcast
        
        //itcast是spider的name
        //成功生成源代码到teacher.html则表示运行成功
            
- items文件中添加需要爬取的内容
    
    - 按照模板添加name、title、info
    
            class MyspiderItem(scrapy.Item):
            # define the fields for your item here like:
            # name = scrapy.Field()
            name = scrapy.Field()
            title = scrapy.Field()
            info = scrapy.Field()
    
- 编写爬虫主配置文件itcast.py
    - 利用选择器（xpath\css）进行内容定位
 
    - 编写parse文件进行response处理

            def parse(self, response):
                teachers = response.xpath("//div[@class='li_txt']")
                for teacher in teachers:
                    item = MyspiderItem()
                    name = teacher.css('h3::text').get()
                    title = teacher.css('h4::text').get()
                    info = teacher.css('p::text').get()
                    item['name'] = name
                    item['title'] = title
                    item['info'] = info
                    yield item
        
    - 再次运行查看是否正常输出item
        >scrapy crawl itcast -o itcast.csv
        
        // -o 命令指定将item保存到csv文件
        
- 编写pipelines.py处理items
    - 可以自定义处理函数： 
        - 对item字段进行修改
        - 可以将item保存到mongodb
                from scrapy.exceptions import DropItem    #导入异常处理模块
                import pymongo
                
                # 省略老师的介绍，最多20个文字
                class InfoPipeline(object):
                    def __init__(self):
                        self.limit = 20
                    def process_item(self, item, spider):
                        if item['info']:
                            if len(item['info']) > self.limit:
                                print('截去超出部分')
                                item['info'] = item['info'][0:self.limit] + '...'
                            return item
                        else:
                            return DropItem('Missing info')
                # 存入mongodb
                class MongoPipeline(object):
                    print('开始存入mongodb')
                    def __init__(self, mongo_uri, mongo_db):
                        self.mongo_uri = mongo_uri
                        self.mongo_db = mongo_db
                    @classmethod
                    def from_crawler(cls, crawler):   # 此函数可以传setting的设置
                        return cls(
                            mongo_uri=crawler.settings.get('MONGO_URI'),
                            mongo_db=crawler.settings.get('MONGO_DB')
                        )
                    def open_spider(self, spider):
                        self.client = pymongo.MongoClient(self.mongo_uri)
                        self.db = self.client[self.mongo_db]
                    def process_item(self, item, spider):
                        name = item.__class__.__name__
                        self.db[name].insert(dict(item))
                        return item  
                    def close_spider(self, spider):
                        self.client.close()
        
        - 编辑完成还需要在settings文件中配置激活
        
- settings设置
    - 编辑settings.py            
    - 找到pipelines相关配置，取消注释，设置mongodb参数等
    
            添加
            # MONGO_CONFIG
            MONGO_URI='localhost'
            MONGO_DB='itcast'
            
            取消注释，将模块名称与pipelines中的一一对应，
            ITEM_PIPELINES = {
               'Myspider.pipelines.InfoPipeline': 200,   //数字代表优先级，越小越优先
               'Myspider.pipelines.MongoPipeline': 350,
            }
                
    - 运行，查看mongodb
    
![Aaron Swartz](https://github.com/Yan199405/Python_note/blob/master/cui-spider/images/al.png)        
            
                    
        
        
        
        
        
        
        
        
        
        
        
               